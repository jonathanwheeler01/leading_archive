{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9o7ZKdqW936"
   },
   "source": [
    "# Google Colab Exercise with PySpark\n",
    "\n",
    "## Purpose\n",
    "\n",
    "In this exercise, we are going to set PySpark with Google Colab and test some of its functionality.\n",
    "\n",
    "From a coding perspective, PySpark may look just like what you can do with sklearn or pandas. So why is the similarity? What is the purpose of using Spark? You may think about these questions while doing the practice.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's do the setup before using PySpark in Colab.\n",
    "\n",
    "### Google Drive Access\n",
    "\n",
    "Before we can install related packages for PySpark, we need to set up proper access to Google Drive from the Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7ViaricW2G3",
    "outputId": "d9456ca2-de2b-49ac-85ac-f568487df874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j3TO8wAETLt"
   },
   "source": [
    "After mounting the Google Drive, visit `Files` in Colab to see if you can access related folders properly. Place a zip file in a directory and test the `unzip` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXGiBdf7X0-I",
    "outputId": "b71343f4-f074-4863-b2db-6a76b12436c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/LEADING2021/test.txt.zip\n",
      "  inflating: /content/drive/MyDrive/LEADING2021/test.txt  \n",
      "   creating: /content/drive/MyDrive/LEADING2021/__MACOSX/\n",
      "  inflating: /content/drive/MyDrive/LEADING2021/__MACOSX/._test.txt  \n"
     ]
    }
   ],
   "source": [
    "# Test google drive access\n",
    "# Suppose you have uploaded a zip file, test.txt.zip, in the MyDrive/LEADING2021 folder\n",
    "!unzip \"/content/drive/MyDrive/LEADING2021/test.txt.zip\" -d \"/content/drive/MyDrive/LEADING2021/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdfP43_pW75h"
   },
   "source": [
    "### PySpark Setup in Colab\n",
    "\n",
    "Now that you have access to Google drive, let's start with setting up PySpark.\n",
    "\n",
    "Note that:\n",
    "\n",
    "1. Each step may take some time; so be patient.\n",
    "2. The latest Spark version may have changed when you see this tutorial. You may visit: https://downloads.apache.org/spark/ first to find out about the latest version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYglDB8UZuLq"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnBZUKGUZ1ut"
   },
   "outputs": [],
   "source": [
    "# add `-P /content/drive/MyDrive/LEADING2021/` if you want to download it to a specific directory\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-D5h__gaFD8"
   },
   "outputs": [],
   "source": [
    "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSlEzcuJaS-e"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtCQNuKXc49J"
   },
   "source": [
    "### Test PySpark\n",
    "\n",
    "If you encountered no error in the previous steps, Spark-related libraries should be in place for Colab.\n",
    "\n",
    "Now let's test it after the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qda0rOlqc1NA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"   # make sure the version number matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UHahd0YdXbS"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "66akFUsFddZy",
    "outputId": "2cf44260-f136-4aba-d6e2-affa1a4cb61c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/spark-3.1.2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDbmxBU6F7bR"
   },
   "source": [
    "If `findspark.find()` output the correct spark version, the setup is ready. We can start a new (local) Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nrDp0w-deuh"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"LEADING\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "JBT-lmUId1H3",
    "outputId": "8e3cd872-5325-4a0c-d058-d33d99997679"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6154de76945c:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LEADING</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f36e7c94450>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8vJjWFBd9cs"
   },
   "source": [
    "## Spark Text Processing\n",
    "\n",
    "Now let's use PySpark to help solve a real-world Big Data problem. In fact, we will be testing it on a very tiny amount of data for the sake of demonstration here.\n",
    "\n",
    "### Text Vectorization\n",
    "\n",
    "We can use related modules in pyspark.ml.feature to extract, transform and select features. In the following example, we use the `Tokenizer` to tokenize some text data and vectorize them using an `IDF` representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29xm-s4uHgK-"
   },
   "source": [
    "1. Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRDWGyLdd4qo",
    "outputId": "31e64db8-432f-4299-c545-a0829c7a6b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|     Python SQL Data| [python, sql, data]|\n",
      "|  0.0|       R SQL Science|   [r, sql, science]|\n",
      "|  1.0|Data Sience and D...|[data, sience, an...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Python SQL Data\"),\n",
    "    (0.0, \"R SQL Science\"),\n",
    "    (1.0, \"Data Sience and Data Engineering\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnupdAEOHjA9"
   },
   "source": [
    "2. Text Vectorization with Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rvkXsrmgGlm"
   },
   "outputs": [],
   "source": [
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "# featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.setInputCol(\"words\")\n",
    "cv.setOutputCol(\"vectors\")\n",
    "model = cv.fit(wordsData)\n",
    "vectorData = model.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aooOTmtRChOA",
    "outputId": "e685ff00-b39d-4e9c-94d9-63dcdd0e1f56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'sql', 'sience', 'python', 'science', 'r', 'engineering', 'and']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af1lGWH2DJQt",
    "outputId": "ec77591d-5e24-4608-9d1d-364eaa715a57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vectors=SparseVector(8, {0: 1.0, 1: 1.0, 3: 1.0}))"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorData.select(\"vectors\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHQzVp1fHrU2"
   },
   "source": [
    "3. IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhScjUrgg9Bh"
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"vectors\", outputCol=\"idfs\")\n",
    "idfModel = idf.fit(vectorData)\n",
    "idfData = idfModel.transform(vectorData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hUZV52LFCEl",
    "outputId": "cceda62e-2bb5-47b8-8fec-ea0550fd6cb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(idfs=SparseVector(8, {0: 0.2877, 1: 0.2877, 3: 0.6931}))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idfData.select(\"idfs\").show()\n",
    "idfData.select(\"idfs\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3djroQS7IWdm"
   },
   "source": [
    "### Parallel Computing\n",
    "\n",
    "We have used PySpark to process text in a local, in-memory session. The same can be done using other packages such as NLTK and/or sklearn. So why do we use Spark?\n",
    "\n",
    "The benefit of Spark is that we can run the same process on a computer cluster, where work can be distributed/parallized. This is especially useful when we have a lot of data (volume and velocity) and the computing is intensive.\n",
    "\n",
    "#### Continued \"Words\" Example in Memory\n",
    "\n",
    "In the above example, we have a list of tokenized words in the `words` column. Let's take a quick look at how the data can be distributed using the Resilient Distributed Dataset (RDD) so they can be processed in parallel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_4YTV2eFFgI",
    "outputId": "63bc17a9-a84b-4487-fee6-dda9b3ec6c88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[words: array<string>]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = wordsData.select(\"words\")\n",
    "# words.show()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xAGKtPNInfv"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = spark.sparkContext # pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMeeZQ6FJSoj"
   },
   "outputs": [],
   "source": [
    "#words2 = \"to be or not to be\".split()\n",
    "words2 = list(words.toPandas()['words'])\n",
    "words_rdd = sc.parallelize(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QtrI4gkKUnl",
    "outputId": "cf311e06-502a-4ac0-8e78-96a04ec64d3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[28] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sIvPrdlL6J_",
    "outputId": "e624cd74-2aa8-42ed-895d-9750c27178b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuples_rdd = words_rdd.map(lambda x: (x, 1))\n",
    "word_tuples_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKj057usL8v7",
    "outputId": "4973eb61-a398-4229-9358-00a459d5c23f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['python', 'sql', 'data'], 1),\n",
       " (['r', 'sql', 'science'], 1),\n",
       " (['data', 'sience', 'and', 'data', 'engineering'], 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuples_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERmHS9MwOc_Y"
   },
   "source": [
    "#### Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFrp76qeJ85Q"
   },
   "source": [
    "In reality, it is more likely we have the distributed data as data files and we need to load them into memory before processing them. And we can implement the processes of `map` and `reduce` to run certain statistics, e.g. to get word counts.\n",
    "\n",
    "Let's create a simple data file, e.g. `words.txt` on the google drive, and put some text content in it. The example here only uses one file; but remember one may be processing many files from many computing nodes of a Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGIkxVuYKlnp"
   },
   "source": [
    "Now can load the text data using `textFile()` of the Spark context and then split the text into words as RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHUpoziZMxbe"
   },
   "outputs": [],
   "source": [
    "words3 = sc.textFile(\"/content/drive/MyDrive/LEADING2021/words.txt\")\\\n",
    "            .flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0LKWq_QLJEV"
   },
   "source": [
    "Now we use `collect` to pull all data together from all nodes, though in this case we only have one, local Spark node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0N_VmzFO5Pf",
    "outputId": "259fad9a-88da-48d5-9beb-25fb1befc031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'SQL',\n",
       " 'Data',\n",
       " 'R',\n",
       " 'SQL',\n",
       " 'Science',\n",
       " 'Data',\n",
       " 'Sience',\n",
       " 'and',\n",
       " 'Data',\n",
       " 'Engineering']"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G31_zAwWMuLU"
   },
   "source": [
    "Now we use:\n",
    "\n",
    "1. `map` to reduce every word into lower-case form and produce `word 1` as `key value/count` pairs.\n",
    "2. `reduce` to aggregate the count for each word/key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gz4K2Q7O_kr"
   },
   "outputs": [],
   "source": [
    "wordCounts = words3.map(lambda word: (word.lower(), 1))\\\n",
    "                    .reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiwVTpwPNgmL"
   },
   "source": [
    "In the end, `collect` the aggregated data, i.e. `wordCounts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCEvFA3KPhPo",
    "outputId": "5376647e-f4a1-4983-d129-3d9f0d9738ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 1),\n",
       " ('sql', 2),\n",
       " ('data', 3),\n",
       " ('r', 1),\n",
       " ('science', 1),\n",
       " ('sience', 1),\n",
       " ('and', 1),\n",
       " ('engineering', 1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pOeCqF-Np4J"
   },
   "source": [
    "We save the output to a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_96ybsvPjWH"
   },
   "outputs": [],
   "source": [
    "wordCounts.saveAsTextFile(\"/content/drive/MyDrive/LEADING2021/word_counts2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vk3NTRYQPumq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA7mNe5V6t2y"
   },
   "source": [
    "## Spark Structured Data Processing\n",
    "\n",
    "Now let's take a look at structured data processing with Spark, using the classic `Iris` dataset in CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bOx0C-473BG",
    "outputId": "5a18017d-17e0-49d6-a881-fb382131cdb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: double (nullable = true)\n",
      " |-- SepalWidthCm: double (nullable = true)\n",
      " |-- PetalLengthCm: double (nullable = true)\n",
      " |-- PetalWidthCm: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = spark.read.csv(\"/content/drive/MyDrive/LEADING2021/iris.csv\", header=True, inferSchema=True)\n",
    "iris.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMA_BfZJ8AT1",
    "outputId": "bd7dafc5-f9a8-4959-d107-aaf5965ba0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0H0XcEhm9fRv",
    "outputId": "f154c266-d67f-43ba-8be9-cc1d113e44be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Yk8MLwF-FkW"
   },
   "outputs": [],
   "source": [
    "iris.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dhFgpoR-mnk"
   },
   "source": [
    "### Spark SQL\n",
    "\n",
    "PySpark includes functions to query structured data on Spark, using DataFrame APIs or raw SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPC3lKql99-H",
    "outputId": "acc9fbc9-79f7-4c03-a2aa-5d12e890c56e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SepalLengthCm: double, SepalWidthCm: double]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare to `select SepalLengthCm, SepalWidthCm from iris` in SQL\n",
    "iris.select(\"SepalLengthCm\", \"SepalWidthCm\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3-vyDWG-JF3",
    "outputId": "1b0d6754-0836-44f9-d86f-74573c78f115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|        Species|\n",
      "+---------------+\n",
      "| Iris-virginica|\n",
      "|    Iris-setosa|\n",
      "|Iris-versicolor|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare to `select distinct Species from iris\" in SQL\n",
    "iris.select('Species').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCD3QaadAUgt"
   },
   "outputs": [],
   "source": [
    "iris2 = iris.filter(iris.PetalWidthCm>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJPyH4hVA5pU",
    "outputId": "322758a7-e95c-4510-feab-7421d0d30c80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBgpAcMnBhhi",
    "outputId": "6ecf0b3d-f6b8-4c08-d807-f0c36684e44c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_species = ['Iris-setosa', 'Iris-virginica']\n",
    "iris.filter( (iris.Species.isin(two_species)) & (iris.PetalWidthCm>1) )\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHANEAhqvV7C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N47nSxMCwV89"
   },
   "source": [
    "#### Group By\n",
    "\n",
    "Compute aggregations based on groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMUGSrSD-WK9",
    "outputId": "68d7261d-6d7d-45b4-e37c-3d638cff2198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|        Species|count(Id)|\n",
      "+---------------+---------+\n",
      "| Iris-virginica|       50|\n",
      "|    Iris-setosa|       50|\n",
      "|Iris-versicolor|       50|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Compare to SQL like:\n",
    "# select Species, count(Id) from iris_table\n",
    "iris.groupBy(\"Species\").agg(F.count(\"Id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvrKq4Hbx76P"
   },
   "source": [
    "Use filter to obtain a subset and then run group-by statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-jULKw4_Jf3",
    "outputId": "836f0ba2-791c-4d27-bc7e-306be35ba8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|       Species|count(Id)|\n",
      "+--------------+---------+\n",
      "|Iris-virginica|       50|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.filter( (iris.Species.isin(two_species)) & (iris.PetalWidthCm>1) )\\\n",
    "  .groupBy(\"Species\").agg(F.count(\"Id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNH-Ms8OwddI"
   },
   "source": [
    "#### RAW SQL\n",
    "\n",
    "It is also possible to run raw SQL statements directly within the Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9IaOdFbCda4"
   },
   "outputs": [],
   "source": [
    "# Register the iris dataframe as a table\n",
    "iris.registerTempTable(\"iris\")\n",
    "\n",
    "# Query the registered table `iris`\n",
    "sc.sql(\"select Species from iris\").show(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
