{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8uFV8NLFtSq"
   },
   "source": [
    "This notebook demonstrates stemming. To get started, we first need to install the tokenizer dependency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyzSwWMyFoZt",
    "outputId": "dfe6b2cf-cda6-4f1e-9b23-de3610e0891d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvN87ivBF9Id"
   },
   "source": [
    "This `punkt` tokenizer divides a text into a list of sentences\n",
    "by using an unsupervised algorithm to build a model for abbreviation\n",
    "words, collocations, and words that start sentences. Next, we import the stemmers from `nltk` and define documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCR1RRotFUSh",
    "outputId": "ece5cdc6-de8d-44d9-de6d-049c3489e921"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over the lazy dog. résumé',\n",
       " 'The sum of the square of the legs of a right triangle are equal to the square of the hypotenuse. resume',\n",
       " 'We are what we pretend to be, so we must be careful about what we pretend to be.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.stem.lancaster import *\n",
    "\n",
    "d1 = \"The quick brown fox jumped over the lazy dog. résumé\"\n",
    "d2 = \"The sum of the square of the legs of a right triangle are equal to the square of the hypotenuse. resume\"\n",
    "d3 = \"We are what we pretend to be, so we must be careful about what we pretend to be.\"\n",
    "docs = [d1, d2, d3]\n",
    "\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37HE_hQMGE6z"
   },
   "source": [
    "The stemmers each need to be initialized to be used in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6s1z_WaGKrJ",
    "outputId": "e6c3b259-6a70-4977-fd48-ec5847fe6624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\") # aka Porter2\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "stemmer = porter_stemmer  # Change to any of the above\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz1CuG49GQRM"
   },
   "source": [
    "Next we tokenize each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBbXq4NdGSy9",
    "outputId": "f7122693-f54e-4364-93ac-f4e5b2c43d5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'quick',\n",
       "  'brown',\n",
       "  'fox',\n",
       "  'jumped',\n",
       "  'over',\n",
       "  'the',\n",
       "  'lazy',\n",
       "  'dog',\n",
       "  '.',\n",
       "  'résumé'],\n",
       " ['The',\n",
       "  'sum',\n",
       "  'of',\n",
       "  'the',\n",
       "  'square',\n",
       "  'of',\n",
       "  'the',\n",
       "  'legs',\n",
       "  'of',\n",
       "  'a',\n",
       "  'right',\n",
       "  'triangle',\n",
       "  'are',\n",
       "  'equal',\n",
       "  'to',\n",
       "  'the',\n",
       "  'square',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hypotenuse',\n",
       "  '.',\n",
       "  'resume'],\n",
       " ['We',\n",
       "  'are',\n",
       "  'what',\n",
       "  'we',\n",
       "  'pretend',\n",
       "  'to',\n",
       "  'be',\n",
       "  ',',\n",
       "  'so',\n",
       "  'we',\n",
       "  'must',\n",
       "  'be',\n",
       "  'careful',\n",
       "  'about',\n",
       "  'what',\n",
       "  'we',\n",
       "  'pretend',\n",
       "  'to',\n",
       "  'be',\n",
       "  '.']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_list = []\n",
    "for document in docs:\n",
    "   term_list.append(nltk.word_tokenize(document))\n",
    "\n",
    "term_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25D2wqmmGWTF"
   },
   "source": [
    "Then we iterate through each document in the document and stem the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUoqmDWOGYeF",
    "outputId": "55650378-1567-4724-e951-e20b87588f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'résumé']\n",
      "['the', 'sum', 'of', 'the', 'squar', 'of', 'the', 'leg', 'of', 'a', 'right', 'triangl', 'are', 'equal', 'to', 'the', 'squar', 'of', 'the', 'hypotenus', '.', 'resum']\n",
      "['we', 'are', 'what', 'we', 'pretend', 'to', 'be', ',', 'so', 'we', 'must', 'be', 'care', 'about', 'what', 'we', 'pretend', 'to', 'be', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmed_term_list = []\n",
    "for i, term_list_for_doc in enumerate(term_list):\n",
    "   stemmed_term_list.append([stemmer.stem(word) for word in term_list_for_doc])\n",
    "\n",
    "# Print lists of stemmed terms for each document\n",
    "for stemmed_doc in stemmed_term_list:\n",
    "  print(stemmed_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHKKyfmdGeDX"
   },
   "source": [
    "Finally, we re-assemble terms in each document to demonstrate stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaCLxEVoGhk-",
    "outputId": "baafd751-70ac-4347-87a7-625eca6c37db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0 stemmed terms: the quick brown fox jump over the lazi dog .\n",
      "D1 stemmed terms: the sum of the squar of the leg of a right triangl are equal to the squar of the hypotenus .\n",
      "D2 stemmed terms: we are what we pretend to be , so we must be care about what we pretend to be .\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "  stemmed_docs_separated_by_space = ' '.join(stemmed_term_list[i])\n",
    "  print(f'D{i} stemmed terms: {stemmed_docs_separated_by_space}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
